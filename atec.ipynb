{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wave/software/anaconda/envs/atec/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import jieba as jb\n",
    "from gensim.models import Word2Vec\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import *\n",
    "from keras.models import Model\n",
    "from keras.optimizers import RMSprop\n",
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache /tmp/jieba.cache\n",
      "Loading model cost 2.111 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "jb.load_userdict('./dict.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define constant\n",
    "MAX_LEN = 280\n",
    "EMD_DIM = 200\n",
    "\n",
    "with open('stopwords.txt') as f:\n",
    "    STOP_WORDS = f.read().decode('utf-8').split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "data1 = pd.read_csv('atec_nlp_sim_train.csv', sep='\\t', index_col=0, names=['sen1', 'sen2', 'label'])\n",
    "data2 = pd.read_csv('atec_nlp_sim_train_add.csv', sep='\\t', index_col=0, names=['sen1', 'sen2', 'label'])\n",
    "data = pd.concat([data1, data2])\n",
    "\n",
    "sens1, sens2, labels = data['sen1'].as_matrix(), data['sen2'].as_matrix(), data['label'].as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(X1, X2, y, test_size=0.2, shuffle=True):\n",
    "    leng = len(y)\n",
    "    inds = np.arange(leng)\n",
    "    if shuffle:\n",
    "        np.random.shuffle(inds)\n",
    "    hook = int(leng * test_size)\n",
    "    X1_train, X2_train, y_train = X1[inds[:-hook]], X2[inds[:-hook]], y[inds[:-hook]]\n",
    "    X1_test, X2_test, y_test = X1[inds[-hook:]], X2[inds[-hook:]], y[inds[-hook:]]\n",
    "    return X1_train, X2_train, y_train, X1_test, X2_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# discarded\n",
    "class MyTokenizer():\n",
    "    def __init__(self, filters=u'，？。！的了和是就都而及与着或'):\n",
    "        self.filters = set(filters)\n",
    "        self.word_dict = {'<PAD>':0, '<UNK>':1}\n",
    "        \n",
    "    def fit(self, texts):\n",
    "        for seq in texts:\n",
    "            words = jb.lcut(seq)\n",
    "            words = [w for w in words if w.strip() and w not in self.filters]\n",
    "            for w in words:\n",
    "                if w not in self.word_dict:\n",
    "                    self.word_dict[w] = len(self.word_dict)\n",
    "        return self.word_dict\n",
    "    \n",
    "    def texts_to_seqs(self, texts):\n",
    "        result = []\n",
    "        for seq in texts:\n",
    "            words = jb.lcut(seq)\n",
    "            words = [w for w in words if w.strip() and w not in self.filters]\n",
    "            result.append([self.word_dict[w] if w in self.word_dict else 1 for w in words])\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(texts, filters=u'，？。！'):\n",
    "    filters = list(filters) + STOP_WORDS\n",
    "    sens = []\n",
    "    for sen in texts:\n",
    "        words = jb.lcut(sen)\n",
    "        words = [w for w in words if w.strip() and w not in filters]\n",
    "        sens.append(words)\n",
    "    return sens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_int_seqs(seqs, vocab):\n",
    "    rs = []\n",
    "    for s in seqs:\n",
    "        rs.append([vocab[w] if w in vocab else 0 for w in s])\n",
    "    return rs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_emb_matrix(word_vecs, vocab):\n",
    "    emb_matrix = np.zeros((len(vocab), EMD_DIM))\n",
    "    for w,i in vocab.items():\n",
    "        if i != 0:\n",
    "            emb_matrix[i] = word_vecs[w]\n",
    "    return emb_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fscore(y_true, y_pred):\n",
    "    y_pred = K.cast(K.greater(y_pred, 0.5), dtype='float32')\n",
    "    tp = K.sum(K.cast(K.equal(y_true+y_pred, 2.), dtype='float32'))\n",
    "    pred_p = K.sum(y_pred) + K.epsilon()\n",
    "    actual_p = K.sum(y_true) + K.epsilon()\n",
    "    precision = tp / pred_p\n",
    "    recall = tp / actual_p\n",
    "    return (2 * precision * recall) / (precision + recall + K.epsilon())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_model(vocab_size, emb_matrix, lr=0.001):\n",
    "    \n",
    "    input1 = Input(shape=(MAX_LEN,))\n",
    "    input2 = Input(shape=(MAX_LEN,))\n",
    "    \n",
    "    embedding = Embedding(vocab_size,\n",
    "                          EMD_DIM,\n",
    "                          weights=[emb_matrix],\n",
    "                          input_length=MAX_LEN,\n",
    "                          trainable=False)    \n",
    "    conv1d = Conv1D(128, 3)\n",
    "    lstm1 = Bidirectional(LSTM(128, return_sequences=True))\n",
    "    lstm2 = Bidirectional(LSTM(128, return_sequences=True))\n",
    "    pooling1d = MaxPooling1D(pool_size=2)\n",
    "    \n",
    "    x1 = embedding(input1)\n",
    "    x2 = embedding(input2)\n",
    "    \n",
    "    x1 = conv1d(x1)\n",
    "    x2 = conv1d(x2)\n",
    "    \n",
    "    x1 = pooling1d(x1)\n",
    "    x2 = pooling1d(x2)\n",
    "    \n",
    "    x1 = lstm1(x1)\n",
    "    x2 = lstm1(x2)\n",
    "    \n",
    "    #x1 = lstm2(x1)\n",
    "    #x2 = lstm2(x2)\n",
    "    \n",
    "    x3 = Multiply()([x1, x2])    \n",
    "    \n",
    "    x4 = Subtract()([x1, x2])\n",
    "    x4 = Lambda(lambda x: K.abs(x))(x4)\n",
    "    \n",
    "    x = Concatenate(axis=-1)([x3, x4])\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(10)(x)\n",
    "    \n",
    "    pred = Dense(1, activation='sigmoid')(x)\n",
    "    \n",
    "    model = Model(inputs=[input1, input2], outputs=pred)\n",
    "    \n",
    "    model.compile(optimizer=RMSprop(lr=lr),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy', fscore])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split data\n",
    "X1_train, X2_train, y_train, X1_test, X2_test, y_test = train_test_split(sens1, sens2, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1_train, X2_train = tokenize(X1_train), tokenize(X2_train)\n",
    "X1_test, X2_test = tokenize(X1_test), tokenize(X2_test)\n",
    "\n",
    "sentences = np.concatenate((X1_train, X2_train), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv_model = Word2Vec(size=EMD_DIM, min_count=1)\n",
    "wv_model.build_vocab(sentences)\n",
    "wv_model.train(sentences, total_examples=wv_model.corpus_count, epochs=8)\n",
    "wv_model.save('./wv_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors = wv_model.wv\n",
    "del wv_model\n",
    "i2w = [u'<UNK>'] + word_vectors.index2entity\n",
    "vocab = dict(zip(i2w, range(len(i2w))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_paded_seqs = lambda x:pad_sequences(to_int_seqs(x, vocab),\\\n",
    "                    maxlen=MAX_LEN, padding='post', truncating='post')\n",
    "\n",
    "X1_train, X2_train = to_paded_seqs(X1_train), to_paded_seqs(X2_train)\n",
    "X1_test, X2_test = to_paded_seqs(X1_test), to_paded_seqs(X2_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_matrix = gen_emb_matrix(word_vectors, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_17 (InputLayer)           (None, 280)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_18 (InputLayer)           (None, 280)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_9 (Embedding)         (None, 280, 200)     2375200     input_17[0][0]                   \n",
      "                                                                 input_18[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_9 (Conv1D)               (None, 278, 128)     76928       embedding_9[0][0]                \n",
      "                                                                 embedding_9[1][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_9 (MaxPooling1D)  (None, 139, 128)     0           conv1d_9[0][0]                   \n",
      "                                                                 conv1d_9[1][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_17 (Bidirectional (None, 139, 256)     263168      max_pooling1d_9[0][0]            \n",
      "                                                                 max_pooling1d_9[1][0]            \n",
      "__________________________________________________________________________________________________\n",
      "subtract_9 (Subtract)           (None, 139, 256)     0           bidirectional_17[0][0]           \n",
      "                                                                 bidirectional_17[1][0]           \n",
      "__________________________________________________________________________________________________\n",
      "multiply_9 (Multiply)           (None, 139, 256)     0           bidirectional_17[0][0]           \n",
      "                                                                 bidirectional_17[1][0]           \n",
      "__________________________________________________________________________________________________\n",
      "lambda_9 (Lambda)               (None, 139, 256)     0           subtract_9[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 139, 512)     0           multiply_9[0][0]                 \n",
      "                                                                 lambda_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_8 (Flatten)             (None, 71168)        0           concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_12 (Dense)                (None, 10)           711690      flatten_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_13 (Dense)                (None, 1)            11          dense_12[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 3,426,997\n",
      "Trainable params: 1,051,797\n",
      "Non-trainable params: 2,375,200\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = my_model(len(vocab), emb_matrix, lr=0.01)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 81982 samples, validate on 20495 samples\n",
      "Epoch 1/5\n",
      " 1664/81982 [..............................] - ETA: 58:01 - loss: 3.1135 - acc: 0.7686 - fscore: 0.0208"
     ]
    }
   ],
   "source": [
    "model.fit([X1_train, X2_train], y_train, batch_size=64, epochs=5, validation_data=([X1_test, X2_test], y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8162157546778561\n",
      "0.8234691388143449\n"
     ]
    }
   ],
   "source": [
    "print(1 - y_train.sum()*1.0 / len(y_train))\n",
    "print(1 - y_test.sum()*1.0 / len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 以下代码只是辅助找出数据里的一些常用词以补充jieba词典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(sens):\n",
    "    new_sens = []\n",
    "    for s in sens:\n",
    "        new_sens.append(re.sub(u'[，。？！]', ' ', s.decode('utf-8')).strip())\n",
    "    return new_sens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_counts(sens, n, min_count=5, inverse=False):\n",
    "    counts = {}\n",
    "    for s in sens:\n",
    "        if len(s) < n:\n",
    "            continue\n",
    "        if inverse:\n",
    "            s = s[::-1]\n",
    "        for i in range(len(s)-n+1):\n",
    "            k = s[i].strip() if n==1 else tuple(s[i+j] for j in range(n) if s[i+j].strip())\n",
    "            if len(k) != n:\n",
    "                continue\n",
    "            if k in counts:\n",
    "                counts[k] += 1\n",
    "            else:\n",
    "                counts[k] = 1\n",
    "    del_ks = [k for k,v in counts.items() if v<min_count]\n",
    "    for k in del_ks:\n",
    "        del counts[k]\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_words(sens, min_freq=0.8, min_count=5):\n",
    "    \n",
    "    uni_counts = n_counts(sens, 1, min_count)\n",
    "    bi_counts = n_counts(sens, 2, min_count)\n",
    "    tri_counts = n_counts(sens, 3, min_count)\n",
    "    four_counts = n_counts(sens, 4, min_count)\n",
    "    \n",
    "    ibi_counts = n_counts(sens, 2, min_count, inverse=True)\n",
    "    itri_counts = n_counts(sens, 3, min_count, inverse=True)\n",
    "    ifour_counts = n_counts(sens, 4, min_count, inverse=True)\n",
    "    \n",
    "    bi_words = set()\n",
    "    for k,v in bi_counts.items():\n",
    "        freq = v*1.0 / uni_counts[k[0]] if k[0] in uni_counts else 0\n",
    "        if freq >= min_freq:\n",
    "            bi_words.add(u''.join(k))\n",
    "            \n",
    "    tri_words = set()\n",
    "    for k,v in tri_counts.items():\n",
    "        freq = v*1.0 / bi_counts[(k[0],k[1])] if (k[0],k[1]) in bi_counts else 0\n",
    "        if freq >= min_freq:\n",
    "            tri_words.add(u''.join(k))\n",
    "            \n",
    "    four_words = set()\n",
    "    for k,v in four_counts.items():\n",
    "        freq = v*1.0 / tri_counts[(k[0],k[1],k[2])] if (k[0],k[1],k[2]) in tri_counts else 0\n",
    "        if freq >= min_freq:\n",
    "            four_words.add(u''.join(k))\n",
    "            \n",
    "    ibi_words = set()\n",
    "    for k,v in ibi_counts.items():\n",
    "        freq = v*1.0 / uni_counts[k[0]] if k[0] in uni_counts else 0\n",
    "        if freq >= min_freq:\n",
    "            ibi_words.add(u''.join(k[::-1]))\n",
    "            \n",
    "    itri_words = set()\n",
    "    for k,v in itri_counts.items():\n",
    "        freq = v*1.0 / ibi_counts[(k[0],k[1])] if (k[0],k[1]) in ibi_counts else 0\n",
    "        if freq >= min_freq:\n",
    "            itri_words.add(u''.join(k[::-1]))\n",
    "            \n",
    "    ifour_words = set()\n",
    "    for k,v in ifour_counts.items():\n",
    "        freq = v*1.0 / itri_counts[(k[0],k[1],k[2])] if (k[0],k[1],k[2]) in itri_counts else 0\n",
    "        if freq >= min_freq:\n",
    "            ifour_words.add(u''.join(k[::-1]))\n",
    "    \n",
    "    bi_words = list(bi_words & ibi_words)\n",
    "    tri_words = list(tri_words & itri_words)\n",
    "    four_words = list(four_words & ifour_words)\n",
    "    \n",
    "    #bi_words = [w for w in bi_words if not any([w in tri for tri in tri_words])]\n",
    "    #tri_words = [w for w in tri_words if not any([w in four for four in four_words])]\n",
    "    \n",
    "    return bi_words + tri_words + four_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import re\n",
    "newsens = clean(np.concatenate((sens1, sens2)))\n",
    "words = extract_words(newsens, 0.8, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dict.txt', 'w') as f:\n",
    "    f.write(u'\\n'.join(words).encode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
