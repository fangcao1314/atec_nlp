{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wave/software/anaconda/envs/atec/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import jieba as jb\n",
    "from gensim.models import Word2Vec\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import *\n",
    "from keras.models import Model\n",
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.391 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "jb.load_userdict('./userdict.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define constant\n",
    "MAX_LEN = 200\n",
    "EMD_DIM = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "data1 = pd.read_csv('atec_nlp_sim_train.csv', sep='\\t', index_col=0, names=['sen1', 'sen2', 'label'])\n",
    "data2 = pd.read_csv('atec_nlp_sim_train_add.csv', sep='\\t', index_col=0, names=['sen1', 'sen2', 'label'])\n",
    "data = pd.concat([data1, data2])\n",
    "\n",
    "sens1, sens2, labels = data['sen1'].as_matrix(), data['sen2'].as_matrix(), data['label'].as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(X1, X2, y, test_size=0.2, shuffle=True):\n",
    "    leng = len(y)\n",
    "    inds = np.arange(leng)\n",
    "    if shuffle:\n",
    "        np.random.shuffle(inds)\n",
    "    hook = int(leng * test_size)\n",
    "    X1_train, X2_train, y_train = X1[inds[:-hook]], X2[inds[:-hook]], y[inds[:-hook]]\n",
    "    X1_test, X2_test, y_test = X1[inds[-hook:]], X2[inds[-hook:]], y[inds[-hook:]]\n",
    "    return X1_train, X2_train, y_train, X1_test, X2_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# discarded\n",
    "class MyTokenizer():\n",
    "    def __init__(self, filters=u'，？。！的了和是就都而及与着或'):\n",
    "        self.filters = set(filters)\n",
    "        self.word_dict = {'<PAD>':0, '<UNK>':1}\n",
    "        \n",
    "    def fit(self, texts):\n",
    "        for seq in texts:\n",
    "            words = jb.lcut(seq)\n",
    "            words = [w for w in words if w.strip() and w not in self.filters]\n",
    "            for w in words:\n",
    "                if w not in self.word_dict:\n",
    "                    self.word_dict[w] = len(self.word_dict)\n",
    "        return self.word_dict\n",
    "    \n",
    "    def texts_to_seqs(self, texts):\n",
    "        result = []\n",
    "        for seq in texts:\n",
    "            words = jb.lcut(seq)\n",
    "            words = [w for w in words if w.strip() and w not in self.filters]\n",
    "            result.append([self.word_dict[w] if w in self.word_dict else 1 for w in words])\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(texts, filters=u'，？。！的了和是就都而及与着或'):\n",
    "    filters = set(filters)\n",
    "    sens = []\n",
    "    for sen in texts:\n",
    "        words = jb.lcut(sen)\n",
    "        words = [w for w in words if w.strip() and w not in filters]\n",
    "        sens.append(words)\n",
    "    return sens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_int_seqs(seqs, vocab):\n",
    "    rs = []\n",
    "    for s in seqs:\n",
    "        rs.append([vocab[w] if w in vocab else 0 for w in s])\n",
    "    return rs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_emb_matrix(word_vecs, vocab):\n",
    "    emb_matrix = np.zeros((len(vocab), EMD_DIM))\n",
    "    for w,i in vocab.items():\n",
    "        if i != 0:\n",
    "            emb_matrix[i] = word_vecs[w]\n",
    "    return emb_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fscore(y_true, y_pred):\n",
    "    y_pred = K.cast(K.greater(y_pred, 0.5), dtype='float32')\n",
    "    tp = K.sum(K.cast(K.equal(y_true+y_pred, 2.), dtype='float32'))\n",
    "    pred_p = K.sum(y_pred) + 1e-7\n",
    "    actual_p = K.sum(y_true) + 1e-7\n",
    "    precision = tp / pred_p\n",
    "    recall = tp / actual_p\n",
    "    return precision #(K.constant(2) * precision * recall) / (precision + recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_model(vocab_size, emb_matrix):\n",
    "    \n",
    "    input1 = Input(shape=(MAX_LEN,))\n",
    "    input2 = Input(shape=(MAX_LEN,))\n",
    "    \n",
    "    embedding = Embedding(vocab_size,\n",
    "                          EMD_DIM,\n",
    "                          weights=[emb_matrix],\n",
    "                          input_length=MAX_LEN,\n",
    "                          trainable=False)\n",
    "    lstm = LSTM(256, return_sequences=True)\n",
    "    #time_dense = TimeDistributed(Dense(1))\n",
    "    #conv1d = Conv1D(128, 3)\n",
    "    \n",
    "    x1 = embedding(input1)\n",
    "    x2 = embedding(input2)\n",
    "    \n",
    "    #x1 = conv1d(x1)\n",
    "    #x2 = conv1d(x2)\n",
    "    \n",
    "    x1 = lstm(x1)\n",
    "    x2 = lstm(x2)\n",
    "    \n",
    "    #x1 = time_dense(x1)\n",
    "    #x2 = time_dense(x2)\n",
    "    \n",
    "    #x1 = Flatten()(x1)\n",
    "    #x2 = Flatten()(x2)\n",
    "    \n",
    "    #x = Concatenate(axis=-1)([x1, x2])\n",
    "    #x = Dot(axes=-1, normalize=True)([x1, x2])\n",
    "    x = Multiply()([x1, x2])\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(100)(x)\n",
    "    \n",
    "    pred = Dense(1, activation='sigmoid')(x)\n",
    "    #pred = Activation('sigmoid')(x)\n",
    "    #pred = Multiply()([x, x])\n",
    "    \n",
    "    model = Model(inputs=[input1, input2], outputs=pred)\n",
    "    \n",
    "    model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy', fscore])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split data\n",
    "X1_train, X2_train, y_train, X1_test, X2_test, y_test = train_test_split(sens1, sens2, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1_train, X2_train = tokenize(X1_train), tokenize(X2_train)\n",
    "X1_test, X2_test = tokenize(X1_test), tokenize(X2_test)\n",
    "\n",
    "sentences = np.concatenate((X1_train, X2_train), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv_model = Word2Vec(size=EMD_DIM, min_count=1)\n",
    "wv_model.build_vocab(sentences)\n",
    "wv_model.train(sentences, total_examples=wv_model.corpus_count, epochs=5)\n",
    "wv_model.save('./wv_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors = wv_model.wv\n",
    "del wv_model\n",
    "i2w = [u'<UNK>'] + word_vectors.index2entity\n",
    "vocab = dict(zip(i2w, range(len(i2w))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_paded_seqs = lambda x:pad_sequences(to_int_seqs(x, vocab),\\\n",
    "                    maxlen=MAX_LEN, padding='post', truncating='post')\n",
    "\n",
    "X1_train, X2_train = to_paded_seqs(X1_train), to_paded_seqs(X2_train)\n",
    "X1_test, X2_test = to_paded_seqs(X1_test), to_paded_seqs(X2_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_matrix = gen_emb_matrix(word_vectors, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_27 (InputLayer)           (None, 200)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_28 (InputLayer)           (None, 200)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_14 (Embedding)        (None, 200, 200)     2401800     input_27[0][0]                   \n",
      "                                                                 input_28[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lstm_14 (LSTM)                  (None, 200, 256)     467968      embedding_14[0][0]               \n",
      "                                                                 embedding_14[1][0]               \n",
      "__________________________________________________________________________________________________\n",
      "multiply_6 (Multiply)           (None, 200, 256)     0           lstm_14[0][0]                    \n",
      "                                                                 lstm_14[1][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)             (None, 51200)        0           multiply_6[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_12 (Dense)                (None, 100)          5120100     flatten_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_13 (Dense)                (None, 1)            101         dense_12[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 7,989,969\n",
      "Trainable params: 5,588,169\n",
      "Non-trainable params: 2,401,800\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = my_model(len(vocab), emb_matrix)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 81982 samples, validate on 20495 samples\n",
      "Epoch 1/5\n",
      "81982/81982 [==============================] - 5344s 65ms/step - loss: 0.4426 - acc: 0.8177 - fscore: 0.3444 - val_loss: 0.4253 - val_acc: 0.8182 - val_fscore: 0.4918\n",
      "Epoch 2/5\n",
      "81982/81982 [==============================] - 5339s 65ms/step - loss: 0.4062 - acc: 0.8244 - fscore: 0.5314 - val_loss: 0.4141 - val_acc: 0.8242 - val_fscore: 0.5336\n",
      "Epoch 3/5\n",
      "81982/81982 [==============================] - 5073s 62ms/step - loss: 0.3811 - acc: 0.8338 - fscore: 0.6032 - val_loss: 0.4189 - val_acc: 0.8248 - val_fscore: 0.5681\n",
      "Epoch 4/5\n",
      "19264/81982 [======>.......................] - ETA: 59:15 - loss: 0.3508 - acc: 0.8477 - fscore: 0.6580"
     ]
    }
   ],
   "source": [
    "model.fit([X1_train, X2_train], y_train, batch_size=64, epochs=5, validation_data=([X1_test, X2_test], y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8162157546778561\n",
      "0.8234691388143449\n"
     ]
    }
   ],
   "source": [
    "print(1 - y_train.sum()*1.0 / len(y_train))\n",
    "print(1 - y_test.sum()*1.0 / len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
