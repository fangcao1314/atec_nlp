{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wave/software/anaconda/envs/atec/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import jieba as jb\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import *\n",
    "from keras.models import Model\n",
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "print(K.equal(K.constant([1,2,3]), K.constant([0,2,2])).shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.315 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "jb.load_userdict('./userdict.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define constant\n",
    "MAX_LEN = 150\n",
    "EMD_DIM = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "data1 = pd.read_csv('atec_nlp_sim_train.csv', sep='\\t', index_col=0, names=['sen1', 'sen2', 'label'])\n",
    "data2 = pd.read_csv('atec_nlp_sim_train_add.csv', sep='\\t', index_col=0, names=['sen1', 'sen2', 'label'])\n",
    "data = pd.concat([data1, data2])\n",
    "#data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sens1, sens2, labels = data['sen1'].as_matrix(), data['sen2'].as_matrix(), data['label'].as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(X1, X2, y, test_size=0.2, shuffle=True):\n",
    "    leng = len(y)\n",
    "    inds = np.arange(leng)\n",
    "    if shuffle:\n",
    "        np.random.shuffle(inds)\n",
    "    hook = int(leng * test_size)\n",
    "    X1_train, X2_train, y_train = X1[inds[:-hook]], X2[inds[:-hook]], y[inds[:-hook]]\n",
    "    X1_test, X2_test, y_test = X1[inds[-hook:]], X2[inds[-hook:]], y[inds[-hook:]]\n",
    "    return X1_train, X2_train, y_train, X1_test, X2_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyTokenizer():\n",
    "    def __init__(self, filters=u'，？。！'):\n",
    "        self.filters = set(filters)\n",
    "        self.word_dict = {'<PAD>':0, '<UNK>':1}\n",
    "        \n",
    "    def fit(self, texts):\n",
    "        for seq in texts:\n",
    "            words = jb.lcut(seq)\n",
    "            words = [w for w in words if w.strip() and w not in self.filters]\n",
    "            for w in words:\n",
    "                if w not in self.word_dict:\n",
    "                    self.word_dict[w] = len(self.word_dict)\n",
    "        return self.word_dict\n",
    "    \n",
    "    def texts_to_seqs(self, texts):\n",
    "        result = []\n",
    "        for seq in texts:\n",
    "            words = jb.lcut(seq)\n",
    "            words = [w for w in words if w.strip() and w not in self.filters]\n",
    "            result.append([self.word_dict[w] if w in self.word_dict else 1 for w in words])\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fscore(y_true, y_pred):\n",
    "    y_pred = K.cast(K.greater(y_pred, 0.5), dtype='float32')\n",
    "    tp = K.sum(K.cast(K.equal(y_true+y_pred, 2.), dtype='float32'))\n",
    "    pred_p = K.sum(y_pred) + 1e-7\n",
    "    actual_p = K.sum(y_true) + 1e-7\n",
    "    precision = tp / pred_p\n",
    "    recall = tp / actual_p\n",
    "    return precision #(K.constant(2) * precision * recall) / (precision + recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_model(vocab_size):\n",
    "    \n",
    "    input1 = Input(shape=(MAX_LEN,))\n",
    "    input2 = Input(shape=(MAX_LEN,))\n",
    "    \n",
    "    embedding = Embedding(vocab_size, EMD_DIM, input_length=MAX_LEN)\n",
    "    lstm = LSTM(128) # , return_sequences=True)\n",
    "    #time_dense = TimeDistributed(Dense(1))\n",
    "    \n",
    "    x1 = embedding(input1)\n",
    "    x2 = embedding(input2)\n",
    "    \n",
    "    x1 = lstm(x1)\n",
    "    x2 = lstm(x2)\n",
    "    \n",
    "    #x1 = time_dense(x1)\n",
    "    #x2 = time_dense(x2)\n",
    "    \n",
    "    #x1 = Flatten()(x1)\n",
    "    #x2 = Flatten()(x2)\n",
    "    \n",
    "    x = Concatenate(axis=-1)([x1, x2])\n",
    "    #x = Dot(axes=-1, normalize=False)([x1, x2])\n",
    "    \n",
    "    pred = Dense(1, activation='sigmoid')(x)\n",
    "    #pred = Activation('sigmoid')(x)\n",
    "    \n",
    "    model = Model(inputs=[input1, input2], outputs=pred)\n",
    "    \n",
    "    model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy', fscore])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split data\n",
    "X1_train, X2_train, y_train, X1_test, X2_test, y_test = train_test_split(sens1, sens2, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize\n",
    "tokenizer = MyTokenizer()\n",
    "word_dict = tokenizer.fit(np.concatenate((X1_train, X2_train), axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1_train, X2_train = tokenizer.texts_to_seqs(X1_train), tokenizer.texts_to_seqs(X2_train)\n",
    "X1_test, X2_test = tokenizer.texts_to_seqs(X1_test), tokenizer.texts_to_seqs(X2_test)\n",
    "\n",
    "f_pad = lambda x:pad_sequences(x, maxlen=MAX_LEN, padding='post', truncating='pre')\n",
    "X1_train, X2_train = f_pad(X1_train), f_pad(X2_train)\n",
    "X1_test, X2_test = f_pad(X1_test), f_pad(X2_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_45 (InputLayer)           (None, 150)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_46 (InputLayer)           (None, 150)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_23 (Embedding)        (None, 150, 200)     2406000     input_45[0][0]                   \n",
      "                                                                 input_46[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lstm_23 (LSTM)                  (None, 128)          168448      embedding_23[0][0]               \n",
      "                                                                 embedding_23[1][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 256)          0           lstm_23[0][0]                    \n",
      "                                                                 lstm_23[1][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_25 (Dense)                (None, 1)            257         concatenate_5[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 2,574,705\n",
      "Trainable params: 2,574,705\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = my_model(len(word_dict))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 5000 samples\n",
      "Epoch 1/5\n",
      "50000/50000 [==============================] - 1188s 24ms/step - loss: 0.4759 - acc: 0.8189 - fscore: 0.0000e+00 - val_loss: 0.4840 - val_acc: 0.8146 - val_fscore: 0.0000e+00\n",
      "Epoch 2/5\n",
      "50000/50000 [==============================] - 1171s 23ms/step - loss: 0.4739 - acc: 0.8189 - fscore: 0.0000e+00 - val_loss: 0.4797 - val_acc: 0.8146 - val_fscore: 0.0000e+00\n",
      "Epoch 3/5\n",
      "33024/50000 [==================>...........] - ETA: 6:28 - loss: 0.4730 - acc: 0.8193 - fscore: 0.0000e+00"
     ]
    }
   ],
   "source": [
    "model.fit([X1_train[:50000], X2_train[:50000]], y_train[:50000], batch_size=64, epochs=5, validation_data=([X1_test[:5000], X2_test[:5000]], y_test[:5000]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8162157546778561\n",
      "0.8234691388143449\n"
     ]
    }
   ],
   "source": [
    "print(1 - y_train.sum()*1.0 / len(y_train))\n",
    "print(1 - y_test.sum()*1.0 / len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
